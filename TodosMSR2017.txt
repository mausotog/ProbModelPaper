Other CLG thoughts:

The two-level model would be improved if the insertions model were also weighted
probabilistically; the evaluation would be improved if it actually broke down
the full predictive success based on both models together.

In some places we use 200 projects; other we use 500.  We need to unify this. 

We need to have a proper table describing the Defects4J instances we study; I
asked for this in a todo running up to the deadline but we ran out of time.

Really, we need more real bugs in the evaluation.  In a perfect world, we'd
remove the case study subsection entirely and just have a proper evaluation of
model success on their own, and then repair success using the models (and with
held out test cases), comparing success/quality to the GenProg (or PAR) produced
patches.

One possibility in place of the case study is to run the model version of repair
on the IntroClass for Java benchmarks produced by Martin Monperrus and his group
because those held-out test suites are better.  However, it's not obvious to me
what the result would be (given that we learn the probabilities on real world
code and IntroClass4Java is small student programs).

I don't believe we can compare to SPR/Prophet really because they use machine
learning to tune the probabilities (and also they target C).  Moreover they're
not just applying templates like PAR does, they're expanding out staged
templates (reread the papers if you don't believe me).  But we can make this
clearer, and analytically compare.

Another possibility on comparison is to Nopol, which targets Java, and which has
released results as applied to Defects4J.  I think those results are on GitHub,
dig through Martin Monperrus' website to find links. 

(We won't use his GenProg/Par results though because our GenProg is better).

Small point: Fix the bibtex.


Original Mau Todos:

1) Two-level probabilistic model: It is not clear why this study uses two-level
probabilistic models. Why is generalizing prior work [47] matter?

	Highlight the importance and momentum of the prior work we are extending,
	and how we started from the replacement model and then generalized to the
	mutations model.


(2) It is not clear how the probabilistic models are created. "To build the
Mutation operator probabilistic model we normalize the number of incidences of
each mutation operator observed in our dataset, matched as described in Section
III-B." How could the numbers be normalized? Are the numbers of incidences
matched with instances, e.g., Null Checker?

	Be more clear about the numbers being instance counts of the mut operators,
	and the fact that we use a smoothing technique to account for 0 occurrences.

"These normalized instances comprise a probabilistic model, where mutation
operator probability is weighted by its appearance in the dataset." What does
the `appearance' stands for in this sentence?? What are the input and output of
models? Before reading Section IV. Evaluation, it is difficult to understand how
a model is created and works

	Clarify text.

In addition, is Table II the model? Table 2 just shows the frequency rate of
subsequent statements of the return statement. How could this table be the
model? Could this model be generalized in the form of mathematical equation or
follows a certain type distribution? It is not quite sure if ! we can call Table
II a model. Counting incidences is a part of steps to come up with an actual
probabilistic model. After counting the frequency, the generalized model must be
defined. Table II seems just an initial step to do this and there is no actual
model is defined in this study. Using Table II is just an approach to find the
best possible mutation operator that can have higher chance to generate
acceptable patches but does not look like a model.
 
	Be very explicit that Table II is an example of a single line, and not the model.

(1) What are baselines for this study? Table III just shows the comparison
results between replacement probabilistic models and equally distributed
probabilistic models. In related work, "unlike the previous work, we apply this
knowledge when actually creating patch candidates rather than when evaluating
them." Since the key idea of this proposed approach is actually from the
previous work such as [8], [31]. The difference is when the knowledge is
applied. Thus, this study must set them as baselines. Are applying the knowledge
when creating patch candidates are better than that when evaluating them? This
must be a research question to validate. Comparing the proposed probabilistic
model with equally distributed models does not provide any interesting aspect
since it is quite obvious that the proposed probabilistic model could be better
than equally distributed model in some sense.

	We can make the argument that we cannot compare our approach directly with,
	Prophet or SPR, because these target C, therefore we can't use their
	benchmarks. But PAR is a superset of these mutations, and we do are
	comparing against PAR.

(2) This study needs to clearly define evaluation measures to show how and why
using probabilistic models are better to find better mutation operator to
improve APR approach. For example, is there any measure to show how the search
space was reduced after using the models? Does the proposed approach reduce more
search space compared with the existing approaches such as [8] and [31]? How
could the number of variants be a measure in what sense?

	The search space is not reduced in number of variants. But it is reduced in
	the sense that the variants created are more likely to be correct, 

    --CLG: this is an interesting claim that we don't necessarily directly
      evaluate.  But we could, in the spirit of Fan Long and Martin Rinard's
      weird ICSE 2016 paper.  That is: for a set of (real) bugs, enumerate the
      first 100 proposed patches for each bug according to eahc technique.  See
      how many are "plausible" (pass all test case) versus "correct" (pass held
      out test cases). 

    therefore
	we don't have to traverse so much of the search space because the search
	space that we are creating is already close to the solution, and this is
	reflected by the fact that we found the patch faster.


(1) Random success rate is 100% in all experiments as in Table IV. caption: Does
this mean there is no difference using probabilistic model and equally
distributed model in terms of success rate?
	
	Be more clear about the fact that "success rate" here means that the
	approach found at least one solution for the 20 seeds, and maybe report how
	many of the seeds found a solution?
	
    --CLG: ...I'm glad we're having this convo because that's actually what I
    meant by success rate: percentage of random seeds on which a repair is
    found.

(2) Does the smaller number of variants means that the patches can be found
faster? Howe much faster? With the current experimental setting and
interpretation of results, it is difficult to say using probabilistic model is
faster in what sense.

	Consider instead of writting the number of variants it takes to find a
	patch, to write the percentage of time faster/slower. For example: instead
	of 20 variants against 10 variants. Say 50% faster.
    
    --CLG: I don't really like percentage improvements but if we report both,
      that's fine. 
	
* Other comments and suggestions
- In Section II. B.: a number possible mutation operators >> a number of ...?
- Section III. C: no a) and b) but there are c) and d)?
- Section III. C. c): probablistic >> probabilistic?
- The patch example in Page 5 has a typo?  i > l.size() >> i < l.size()? Otherwise the patch still cause array out of bound exception. Or, the model just predicts ifStatement regardless of having the correct if-condition?
- Figure II in Page 6? Figure II >> Table II?
- Page 6: equally distributed approach As we can derive. No period between "approach" and "As"?
- Section IV. C. 1): We consider for repair a subset >> We consider for repairing a subset?
- noise in fault localizaition >> noise in fault localization
- Section VI: to gether >> together?

	

	
Instead of the random, uniform distribution baseline, shouldn't we compare to
the state-of-the-art? For example, the idea of using human-generated bug fixing
behaviour to inform APR has been explored by past work (e.g., [24]). The novelty
of this work appears to be that instead of learning fix templates from history,
we are now using the frequency of mutation operations to guide a heuristic
search. Couldn't one compare the proposed solutions of each approach to
demonstrate whether one solution outperforms the other?

	I think we can make the argument that we are comparing to the state of the
	art in Java APR techniques, which also is a generalization of SPR and
	Prophet as we explained before. 

	We can maybe focus it more towards we are literally comparing to exactly
	what PAR does. PAR uses an equally distributed way to pick the mutation
	operators from the mutation operators that are able to be applied in a
	particular section of the code. 
	
	If we want to compare to SPR/Prophet, we could just take the PAR templates
	that are a generalization of the SPR transformations and run the experiment
	just with those and not all the PAR templates. It is obviously far from the
	real SPR, because SPR targets C, but is this a acceptable Java
	approximation/way to test SPR on Java? 
	Claire, thoughts on this?

    --CLG: see above.  Uniform makes sense as a comparison to PAR.  We might
      also compare expressive power to Nopol.

- The means and standard deviation values shown in Table IV are not enough.
Could you provide a distribution overview (e.g., boxplots)? Also, could you
perform a statistical hypothesis test (e.g., Wilcoxon signed rank test) and
provide a measure of the effect size (e.g., Cliff's delta)? Currently, the
results look very unconvincing because of the large standard deviation values.

	I can come up with nicer graphs for this.

    --CLG: agreed with the reviewer
  
- The results are provided in terms of iterations, which makes sense; however,
it would also be useful to provide a measure of how much time could be saved.
Hours? Minutes? Seconds?

	This is SO machine dependent that I don't think we should provide these
	numbers unless they are ridiculously high.

	-CLG: agreed

(3) The association rule analysis, while interesting and certainly useful, does
not fit with the theme of this paper. I would recommend either: (a) removing it
or (b) spending more effort to explain how the analysis fit together to solve
one common problem. It currently stands out as a tangential analysis that
doesn't really contribute to the main message of the paper.

	I actually like this section a lot, although I do see why the reviewer makes
	this point. I need to blend it more into the story of the paper.
  
- Cross validation is not used to improve external validity. Instead, it
addresses concerns of internal validity. To address external validity concerns,
one needs to repeat the experiment in several settings.

	True, although that is what cross validation is for: internal validity, for
	external validity we perform the comparison of models.

    --CLG says: make sure the text says the right thing
	
- Don't treat citation markers as nouns (e.g., "following the guidelines by [7],
[14], [45].")

	Fix this.

    --CLG says: I also say this all the time, so please start listening to me.
	
Typos
=-=-=

- "There is a broad diversity of such operators used in *<missing word>*,"
- "to provide necessary background and *situation* our work"
- "can be adapted to this task *task*"
- Several instances of "::"
- "high level *or* redundancy"
- "they are shown in the middle segment of Table I *Middle*."
- "greedily attempt *<missing word>* match"
- "the Replacements probabilistic model *model* describes"
- "unecessary"
- "*a* Expression"
- "for return statements shown in *Figure* II." (I think you mean "Table").
- "eigth"
- "We assess *<missing word>* number"
- "using *<missing word>* held-out test suite"
- "have a confidence of *a* 100%"
- "at least *a* 1% of all the transactions"
- "oour"
- "Note that in *<missing word>* V"

	
the Mutation operator probabilistic model and Replacements probabilistic model
are only drafted: the sentence " These normalized instances comprise a
probablistic model, where mutation operator probability is weighted by its
appearance in the dataset." it is not enough to understand how to build the
operator probabilities and it is not clear how "instances comprise a
probabilistic model".  It is not also enough to say "This model is built
analogously to the mutation operator model, with a focus on replacer and
replacement statement incidence." What do you mean with "with a focus on"?

	I guess explain further how the model was built in this section?
	
Some details can be moved in the appropriate section the ease the reading: for
example, the sentence at pg. 7 "(in our dataset, Template-Based mutations
comprise 29.26% of the corpus; Statement-Edit mutations make up 70.74% of the
corpus)." should be put in the section on the corpus.
	
	Move this.
	
I was expecting the authors compared the performance of their approach with the
state-of-the art APRs by, for example, answering the question: are mutation
operators mined from commits likelier to be performed at a specific code
location than operators identified with state-of-the-art APR? I think the
authors did not answer this question clearly.
	
	I already mentioned making a direct comparison to PAR.
	

	
	
	
	
	
	
	
	
	
	
	
	
	
	
