
----------------------- REVIEW 1 ---------------------
PAPER: 136
TITLE: Using a probabilistic model to predict bug fixes
AUTHORS: Mauricio Soto and Claire Le Goues

Overall evaluation: 2 (accept)

----------- Overall evaluation -----------
The paper presents a program repair approach that is based on a large corpus of bug fixes. The corpus is used to inform the probabilities of the mutation operators, so that the proposed repairs are as close to human bug fixes as possible. The approach is evaluated in several steps: first contrasted with a similar approach, but with uniform probabilities. Then with several state of the art approaches. Then an association rule mining technique to produce large fixes is evaluated.

+ Interesting outlook on the problem
+ Well written paper
+ Solid evaluation and convincing performance

- The association rule mining part is somewhat disconnected from the rest of the paper
- Why only run the evaluation on a subset of the data
- Would have liked to see more data about the corpus.

All in all, this is pretty solid work. The paper is also well written, and describes the approach in a convincing manner. The evaluation is solid and the methodology is well described as well.

There are a few points for improvement and comments that I have, but these are nothing major in the grand scheme of things:

* It would be great to have more data on the corpus, perhaps as an appendix. How frequent are each patterns? Further, it's not clear exactly how many bug fixes are in the corpus, as I couldn't find whether the 100 selected bugs fixes are then filtered (for small enough fixes, etc), or if it is 100 bugs after the filtering. Including this kind of information would make the paper more informative.

* The corpus of bugs for evaluation is larger than the one that is actually used in the evaluation of the paper. Why is that? It unnecessarily weakens the results.

* The evaluation in section 4 A fixes the threshold at 5. It would be instructive to vary it in order to see how the difference between the two approaches (uniform probability and corpus based) in performance vary with the size of the threshold. Does it reach a plateau at some point? Does the uniform approach catches up if the threshold is high enough?

* I was expecting higher performance from GenProg, PAR, and TrpAutoRepair, than the one in figure 6. Is the performance of these approaches in this data set in line with the literature? It would be nice to mention either way.

* Finally, the last part of the paper (the association rule mining part), seems somewhat disconnected from the paper. It seems like it could be another paper in itself, if given proper treatment. So, if I were you, I would add the missing data mentioned above to the paper, and give that second part the proper space it deserves, in a subsequent submission.


----------------------- REVIEW 2 ---------------------
PAPER: 136
TITLE: Using a probabilistic model to predict bug fixes
AUTHORS: Mauricio Soto and Claire Le Goues

Overall evaluation: -2 (reject)

----------- Overall evaluation -----------
[Summary]
This paper presents an approach to predict bug fixes with mutation operators using a probabilistic model. The authors collect mutation operators of bug fixes from 500 most popular Java projects in GitHub to create a probabilistic selection model of mutation operators with their occurrences for predicting bug fixes. They further categorize, compare and evaluate these mutation operators which consist of statement-edit mutations and template-based mutations taken from existing research work: GenProg and PAR, etc. Finally, they mine association rules to analyze context surrounding multi-edit source code changes.

The authors evaluate the effectiveness of the approach by comparing against existing work: GenProg, TrpAutoRepari, PAR, and Nopol. The results show that patches generated by their approach are better than the four tools. They also find that association rules can be used to build 84.6% of multi-edit patches with 90% confidence.


[Comments]
Strength:
+ relevant topic to SANER.

Weakness:
- weak motivation.
- unclear descriptions.
- an unnecessary research question.
- unfair evaluation.

This paper is lack of a convincing motivation. Throughout Section 1, nothing is clearly explaining why a probabilistic model is necessary. There should be motivating examples clarifying the necessity of the probabilistic model.

The title of this paper is “Using a probabilistic model to predict bug fixes”, but the abstract and introduction do not highlight and explain the probabilistic model and how to create and use it to predict bug fixes. The research methodology of this paper does not clearly describe it as well. It should be better to highlight and explain the probabilistic model in this paper. Additionally, when authors build the probabilistic model, they count the number of instances of each mutation operator observed in their dataset. It is more likely to be the recurrent bug fix. It might be biased to regard the recurrence of a mutation operator as its probability.

The title of Section III is “Program repair via a learned probabilistic edit model”, but the authors do not describe how to learn a model in this paper. According to reading this section, the title should be “Program repair via a probabilistic edit model”.

The first research question is “how accurate are our mined models in predicting mutation operators and replacing code across large dataset?”. What are the mined models? It is unclear. The paper presents a probabilistic model to predict bug fixes. According to reading the results, it seems that the authors intend to evaluate the accuracy of identifying mutation operators of their approach. This topic might be prone to source code differencing between consecutive revisions, which is not the topic of this paper and related research work being compared.

In the evaluation process, authors use a held-out test suite to measure the quality of generated patches, which might be biased. At the-state-of-the-art, the quality of automatically a generated patch is measured by the method proposed in paper [1]. The comparison against existing work is also biased, which just focuses on the patches that can be generated by the approach proposed by authors.

————————
[1] Martinez, Matias, Thomas Durieux, Romain Sommerard, Jifeng Xuan, and Martin Monperrus. "Automatic repair of real bugs in java: A large-scale experiment on the defects4j dataset." Empirical Software Engineering (2016): 1-29.


----------------------- REVIEW 3 ---------------------
PAPER: 136
TITLE: Using a probabilistic model to predict bug fixes
AUTHORS: Mauricio Soto and Claire Le Goues

Overall evaluation: 2 (accept)

----------- Overall evaluation -----------
The work introduces a novel approach for automatic program repair based on mining developers commits associated to bug fixing.

The paper presents an approach that maps changes in AST between pre-post bug fixes files to mutations, compute the frequency of mutations and replacements in the dataset at fault locations and use a 10-fold one-out approach to evaluate the accuracy of the resulting probabilistic model.

In addition, the authors discuss multiple edits in their corpus by applying the theory of association rules to their dataset. Such analysis opens to an extension of their approach in case of multiple edits as future work.

The work is interesting and technically sound. I have only few comments that I would like the authors to address:

The discussion on tangle commits (i.e., commits that change code for different purposes, including bug fixes) is addressed by limiting the bug fixes to the commits changing at most three files. This choice is not enough to reduce the risk of selecting changes that are not related to bugs and can affect the overall evaluation of the approach by for example including more changes that history tells and so incrementing involuntarily the ability of the authors’ approach in repairing bugs.
Also the choice of “three files” is a bit arbitrary: The authors could for example compare their choice with some statistical trends for the 500 starred projects or a some inspection on a sample of the dataset.

The method that identifies mutations in developers’ commits does not tell whether it is done manually or automatically and in both cases which technique (e.g., in manual inspection what was the sample, how the authors inspected it etc.).

The Nopol tool is used in the comparison, but is not mentioned in the state-of-the-art tools in section II-B.


-------------------------  METAREVIEW  ------------------------
PAPER: 136
TITLE: Using a probabilistic model to predict bug fixes

This paper has been the object of extensive online discussion. While two reviewers appreciate the way the approach is presented, and the multi-dimensional nature of the evaluation, another reviewer has serious objections on the specific component that compares the approach to the state of the art.

In particular, the most serious issue in this part of the evaluation is that it is carried only on the 9 bugs that the approach manages to fix, while nothing is said about the 54 bugs for which it could not find a fix. A compounding issue is that the paper is unclear on this issue, making this particular detail easy to overlook.

The core of the discussion was about the importance of the previous issue, with two reviewers finding this issue acceptable in the context of a "work in progress" evaluation, and a third reviewer finding it an extremely serious concern. The fact that the issue was not clearly presented was also found to be rather problematic.

A revised version of this paper *must* include at least a summary of the performance of all of the approach on the entire set of 63 bugs to present the performance fairly. In particular, Figure 6 should report of further bugs whose patches have been successfully found and / or validated by with the state of the art methods. Based on this data, the paper could concretely motivate whether the presented approach is complementary to the existing ones.

---
Based on the discussion, and although acknowledging weaknesses in the evaluation, program chairs have decided that the novelty of the approach would make this a valuable contribution for the conference, although results are not as good as they seem. However, the program chairs really urge the authors to clearly state upfront the limitations of their evaluation. Also, as it has been suggested by reviewers, it should be discussed the extent to which this approach has the potential to be complementary to existing ones.